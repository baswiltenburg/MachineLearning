{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "### Set-up for training a U-net convolutional neural network for sementic segmentation ###\n",
    "import os\n",
    "from pathlib import Path\n",
    "os.chdir(str(Path.home())+'/bomenspotter/notebooks/Bas/Scripts')\n",
    "from DataPreprocessing import *\n",
    "from DataCreation import *\n",
    "import CreateResults as cr\n",
    "from Unet import unet\n",
    "from random import sample\n",
    "from owslib.wms import WebMapService\n",
    "import tensorflow\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import backend as keras\n",
    "work_directory = str(Path.home())+\"/bomenspotter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (512, 512) \n",
    "cell_size = 0.25 \n",
    "epsg = 28992   \n",
    "wms = WebMapService('https://geodata.nationaalgeoregister.nl/luchtfoto/rgb/wms?&request=GetCapabilities&service=WMS')\n",
    "#wms = WebMapService('https://geodata.nationaalgeoregister.nl/luchtfoto/infrarood/wms?&request=GetCapabilities&service=WMS')\n",
    "\n",
    "path_training_data = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/training_corrected\"\n",
    "path_mask_data_binair = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/binary_mask\"\n",
    "path_mask_data = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/mask\"\n",
    "path_original_data = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/training\"\n",
    "path_check_images = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/check\"\n",
    "folder_checkpoints = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/check_points\"\n",
    "shapeLocationImages = work_directory + \"/data/n2000_project/shape/H5130/Run2/H5130_totaal.shp\"\n",
    "shapeLocationMask = work_directory + \"/data/n2000_project/shape/H5130/Run2/H5130_totaal_mask.shp\"\n",
    "\n",
    "# Create N2000_Data object\n",
    "dc2 = N2000_Data(wms = wms, image_size = image_size, cell_size = cell_size, epsg = epsg)\n",
    "\n",
    "# Create N2000_DataPreparation object\n",
    "dp2 = N2000_DataPreparation(image_size = image_size, path_training_data = path_training_data, path_mask_data = path_mask_data_binair, path_original_data = path_original_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data aquisition\n",
    "- Download aerial images based on known polygons\n",
    "- Create raster masks (binary and non-binary)\n",
    "- Create and rename check images\n",
    "- Create folder of valid training images with corresponding raster mask\n",
    "- Save image data (bounding boxes) to JSON "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bounding_boxes_features, areas = dc2.getBoundingBoxes(shapeLocation = shapeLocationImages)\n",
    "#bounding_boxes_images = dc2.createImageBoundingBoxes(bounding_boxes_features, areas)\n",
    "#bounding_boxes_images = dc2.createImageBoundingBoxes2(shapeLocation = shapeLocationImages)\n",
    "#dc2.downloadTrainingImages(bounding_boxes_images, path_original_data, name = \"h5130_totaal_512px\", ir = False, years = ['2016', '2017', '2018'])\n",
    "#dc2.saveImageDataToJson(image_directory = path_original_data, bounding_boxes_images = bounding_boxes_images, file_name = \"h5130_totaal_512px.json\", image_size = (512,512))\n",
    "#dc2.createRasterMasks(path_original_data, path_mask_data, shapeLocationMask) \n",
    "#dc2.convertMaskToBinaryMask(src_folder = path_mask_data, dst_folder = path_mask_data_binair)\n",
    "#dc2.createCheckingImages(path_original_data, path_check_images, shapeLocationMask)\n",
    "#dc2.createZipfile(path_check_images, filename = \"h5130_totaal_512px_check.gzip\")\n",
    "#dc2.createZipfile(path_training_data, filename = \"h5130_totaal_512px.gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN FUNCTIONS # \n",
    "\n",
    "#dp2.RenameCheckImages()\n",
    "#dp2.PrepareTrainingData()\n",
    "#dp2.RemoveInvalidData()\n",
    "#dp2.CreateH5_files(\"h5130_totaal_512px.h5\")\n",
    "\n",
    "#x_train, y_train, x_val, y_val, x_test, y_test, filenamesTest = dp2.DevideData2(path_dataset = (path_training_data+\"/h5130_totaal_512px.h5\"))\n",
    "#x_train, y_train, x_val, y_val, x_test, y_test, stats =  dp2.NormalizeData(x_train, y_train, x_val, y_val, x_test, y_test)\n",
    "\n",
    "# Perform data augmentation to improve + increase training dataset\n",
    "# Create N2000_DataAugmentation object\n",
    "#da2 = N2000_DataAugmentation(x_train, y_train)\n",
    "#x_train_hf, y_train_hf = da2.HorizontalFlip(batch_size = 200)\n",
    "#x_train_vf, y_train_vf = da2.VerticalFlip(batch_size = 200)\n",
    "#x_train_rr, y_train_rr = da2.RandomRotation(batch_size = 200)\n",
    "\n",
    "# Merge original training data with data augmentation\n",
    "#x_train_total = np.concatenate([x_train,x_train_hf, x_train_vf, x_train_rr])\n",
    "#y_train_total = np.concatenate([y_train,y_train_hf, y_train_vf, y_train_rr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(path_training_data)\n",
    "dest_folder = work_directory + \"/data/n2000_project/training_images/h5130_Totaal/testing_2016\"\n",
    "for file in files:\n",
    "    year = file.split(\"_\")[1]\n",
    "    if year == '2016':\n",
    "        image_copy = shutil.copy(path_training_data+\"/\"+file,dest_folder+\"/\"+file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create N2000_DataPreparation object\n",
    "dp3 = N2000_DataPreparation(image_size = image_size, path_training_data = dest_folder, path_mask_data = path_mask_data_binair, path_original_data = path_original_data)\n",
    "dp3.CreateH5_files(\"h5130_totaal_2016.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:TensorflowBas]",
   "language": "python",
   "name": "conda-env-TensorflowBas-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
